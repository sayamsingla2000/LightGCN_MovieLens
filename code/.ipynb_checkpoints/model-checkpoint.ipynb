{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--bpr_batch BPR_BATCH] [--recdim RECDIM]\n",
      "                             [--layer LAYER] [--lr LR] [--decay DECAY]\n",
      "                             [--dropout DROPOUT] [--keepprob KEEPPROB]\n",
      "                             [--a_fold A_FOLD] [--testbatch TESTBATCH]\n",
      "                             [--dataset DATASET] [--path PATH]\n",
      "                             [--topks [TOPKS]] [--tensorboard TENSORBOARD]\n",
      "                             [--comment COMMENT] [--load LOAD]\n",
      "                             [--epochs EPOCHS] [--multicore MULTICORE]\n",
      "                             [--pretrain PRETRAIN] [--seed SEED]\n",
      "                             [--model MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/sayamsingla/Library/Jupyter/runtime/kernel-7c23caea-d2d6-4580-a403-ea86522c03c9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayamsingla/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import world\n",
    "import torch\n",
    "from dataloader import BasicDataset\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BasicModel(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(BasicModel, self).__init__()\n",
    "    \n",
    "    def getUsersRating(self, users):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class PairWiseModel(BasicModel):\n",
    "    def __init__(self):\n",
    "        super(PairWiseModel, self).__init__()\n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            users: users list \n",
    "            pos: positive items for corresponding users\n",
    "            neg: negative items for corresponding users\n",
    "        Return:\n",
    "            (log-loss, l2-loss)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class PureMF(BasicModel):\n",
    "    def __init__(self, \n",
    "                 config:dict, \n",
    "                 dataset:BasicDataset):\n",
    "        super(PureMF, self).__init__()\n",
    "        self.num_users  = dataset.n_users\n",
    "        self.num_items  = dataset.m_items\n",
    "        self.latent_dim = config['latent_dim_rec']\n",
    "        self.f = nn.Sigmoid()\n",
    "        self.__init_weight()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        self.embedding_user = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "        print(\"using Normal distribution N(0,1) initialization for PureMF\")\n",
    "        \n",
    "    def getUsersRating(self, users):\n",
    "        users = users.long()\n",
    "        users_emb = self.embedding_user(users)\n",
    "        items_emb = self.embedding_item.weight\n",
    "        scores = torch.matmul(users_emb, items_emb.t())\n",
    "        return self.f(scores)\n",
    "    \n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        users_emb = self.embedding_user(users.long())\n",
    "        pos_emb   = self.embedding_item(pos.long())\n",
    "        neg_emb   = self.embedding_item(neg.long())\n",
    "        pos_scores= torch.sum(users_emb*pos_emb, dim=1)\n",
    "        neg_scores= torch.sum(users_emb*neg_emb, dim=1)\n",
    "        loss = torch.mean(nn.functional.softplus(neg_scores - pos_scores))\n",
    "        reg_loss = (1/2)*(users_emb.norm(2).pow(2) + \n",
    "                          pos_emb.norm(2).pow(2) + \n",
    "                          neg_emb.norm(2).pow(2))/float(len(users))\n",
    "        return loss, reg_loss\n",
    "        \n",
    "    def forward(self, users, items):\n",
    "        users = users.long()\n",
    "        items = items.long()\n",
    "        users_emb = self.embedding_user(users)\n",
    "        items_emb = self.embedding_item(items)\n",
    "        scores = torch.sum(users_emb*items_emb, dim=1)\n",
    "        return self.f(scores)\n",
    "\n",
    "class LightGCN(BasicModel):\n",
    "    def __init__(self, \n",
    "                 config:dict, \n",
    "                 dataset:BasicDataset):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.config = config\n",
    "        self.dataset : dataloader.BasicDataset = dataset\n",
    "        self.__init_weight()\n",
    "\n",
    "    def __init_weight(self):\n",
    "        self.num_users  = self.dataset.n_users\n",
    "        self.num_items  = self.dataset.m_items\n",
    "        self.latent_dim = self.config['latent_dim_rec']\n",
    "        self.n_layers = self.config['lightGCN_n_layers']\n",
    "        self.keep_prob = self.config['keep_prob']\n",
    "        self.A_split = self.config['A_split']\n",
    "        self.embedding_user = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "        if self.config['pretrain'] == 0:\n",
    "#             nn.init.xavier_uniform_(self.embedding_user.weight, gain=1)\n",
    "#             nn.init.xavier_uniform_(self.embedding_item.weight, gain=1)\n",
    "#             print('use xavier initilizer')\n",
    "# random normal init seems to be a better choice when lightGCN actually don't use any non-linear activation function\n",
    "            nn.init.normal_(self.embedding_user.weight, std=0.1)\n",
    "            nn.init.normal_(self.embedding_item.weight, std=0.1)\n",
    "            world.cprint('use NORMAL distribution initilizer')\n",
    "        else:\n",
    "            self.embedding_user.weight.data.copy_(torch.from_numpy(self.config['user_emb']))\n",
    "            self.embedding_item.weight.data.copy_(torch.from_numpy(self.config['item_emb']))\n",
    "            print('use pretarined data')\n",
    "        self.f = nn.Sigmoid()\n",
    "        self.Graph = self.dataset.getSparseGraph()\n",
    "        print(f\"lgn is already to go(dropout:{self.config['dropout']})\")\n",
    "\n",
    "        # print(\"save_txt\")\n",
    "    def __dropout_x(self, x, keep_prob):\n",
    "        size = x.size()\n",
    "        index = x.indices().t()\n",
    "        values = x.values()\n",
    "        random_index = torch.rand(len(values)) + keep_prob\n",
    "        random_index = random_index.int().bool()\n",
    "        index = index[random_index]\n",
    "        values = values[random_index]/keep_prob\n",
    "        g = torch.sparse.FloatTensor(index.t(), values, size)\n",
    "        return g\n",
    "    \n",
    "    def __dropout(self, keep_prob):\n",
    "        if self.A_split:\n",
    "            graph = []\n",
    "            for g in self.Graph:\n",
    "                graph.append(self.__dropout_x(g, keep_prob))\n",
    "        else:\n",
    "            graph = self.__dropout_x(self.Graph, keep_prob)\n",
    "        return graph\n",
    "    \n",
    "    def computer(self):\n",
    "        \"\"\"\n",
    "        propagate methods for lightGCN\n",
    "        \"\"\"       \n",
    "        users_emb = self.embedding_user.weight\n",
    "        items_emb = self.embedding_item.weight\n",
    "        all_emb = torch.cat([users_emb, items_emb])\n",
    "        #   torch.split(all_emb , [self.num_users, self.num_items])\n",
    "        embs = [all_emb]\n",
    "        if self.config['dropout']:\n",
    "            if self.training:\n",
    "                print(\"droping\")\n",
    "                g_droped = self.__dropout(self.keep_prob)\n",
    "            else:\n",
    "                g_droped = self.Graph        \n",
    "        else:\n",
    "            g_droped = self.Graph    \n",
    "        \n",
    "        for layer in range(self.n_layers):\n",
    "            if self.A_split:\n",
    "                temp_emb = []\n",
    "                for f in range(len(g_droped)):\n",
    "                    temp_emb.append(torch.sparse.mm(g_droped[f], all_emb))\n",
    "                side_emb = torch.cat(temp_emb, dim=0)\n",
    "                all_emb = side_emb\n",
    "            else:\n",
    "                all_emb = torch.sparse.mm(g_droped, all_emb)\n",
    "            embs.append(all_emb)\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        #print(embs.size())\n",
    "        light_out = torch.mean(embs, dim=1)\n",
    "        users, items = torch.split(light_out, [self.num_users, self.num_items])\n",
    "        return users, items\n",
    "    \n",
    "    def getUsersRating(self, users):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users.long()]\n",
    "        items_emb = all_items\n",
    "        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n",
    "        return rating\n",
    "    \n",
    "    def getEmbedding(self, users, pos_items, neg_items):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        pos_emb = all_items[pos_items]\n",
    "        neg_emb = all_items[neg_items]\n",
    "        users_emb_ego = self.embedding_user(users)\n",
    "        pos_emb_ego = self.embedding_item(pos_items)\n",
    "        neg_emb_ego = self.embedding_item(neg_items)\n",
    "        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego\n",
    "    \n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        (users_emb, pos_emb, neg_emb, \n",
    "        userEmb0,  posEmb0, negEmb0) = self.getEmbedding(users.long(), pos.long(), neg.long())\n",
    "        reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
    "                         posEmb0.norm(2).pow(2)  +\n",
    "                         negEmb0.norm(2).pow(2))/float(len(users))\n",
    "        pos_scores = torch.mul(users_emb, pos_emb)\n",
    "        pos_scores = torch.sum(pos_scores, dim=1)\n",
    "        neg_scores = torch.mul(users_emb, neg_emb)\n",
    "        neg_scores = torch.sum(neg_scores, dim=1)\n",
    "        \n",
    "        loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
    "        \n",
    "        return loss, reg_loss\n",
    "       \n",
    "    def forward(self, users, items):\n",
    "        # compute embedding\n",
    "        all_users, all_items = self.computer()\n",
    "        # print('forward')\n",
    "        #all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        items_emb = all_items[items]\n",
    "        inner_pro = torch.mul(users_emb, items_emb)\n",
    "        gamma     = torch.sum(inner_pro, dim=1)\n",
    "        return gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
