{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import world\n",
    "from world import cprint\n",
    "from time import time\n",
    "\n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"init dataset\")\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def allPos(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserPosItems(self, users):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserNegItems(self, users):\n",
    "        \"\"\"\n",
    "        not necessary for large dataset\n",
    "        it's stupid to return all neg items in super large dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getSparseGraph(self):\n",
    "        \"\"\"\n",
    "        build a graph in torch.sparse.IntTensor.\n",
    "        Details in NGCF's matrix form\n",
    "        A = \n",
    "            |I,   R|\n",
    "            |R^T, I|\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class LastFM(BasicDataset):\n",
    "    \"\"\"\n",
    "    Dataset type for pytorch \\n\n",
    "    Incldue graph information\n",
    "    LastFM dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, path=\"../data/lastfm\"):\n",
    "        # train or test\n",
    "        cprint(\"loading [last fm]\")\n",
    "        self.mode_dict = {'train':0, \"test\":1}\n",
    "        self.mode    = self.mode_dict['train']\n",
    "        # self.n_users = 1892\n",
    "        # self.m_items = 4489\n",
    "        trainData = pd.read_table(join(path, 'data1.txt'), header=None)\n",
    "        # print(trainData.head())\n",
    "        testData  = pd.read_table(join(path, 'test1.txt'), header=None)\n",
    "        # print(testData.head())\n",
    "        trustNet  = pd.read_table(join(path, 'trustnetwork.txt'), header=None).to_numpy()\n",
    "        # print(trustNet[:5])\n",
    "        trustNet -= 1\n",
    "        trainData-= 1\n",
    "        testData -= 1\n",
    "        self.trustNet  = trustNet\n",
    "        self.trainData = trainData\n",
    "        self.testData  = testData\n",
    "        self.trainUser = np.array(trainData[:][0])\n",
    "        self.trainUniqueUsers = np.unique(self.trainUser)\n",
    "        self.trainItem = np.array(trainData[:][1])\n",
    "        # self.trainDataSize = len(self.trainUser)\n",
    "        self.testUser  = np.array(testData[:][0])\n",
    "        self.testUniqueUsers = np.unique(self.testUser)\n",
    "        self.testItem  = np.array(testData[:][1])\n",
    "        self.Graph = None\n",
    "        print(f\"LastFm Sparsity : {(len(self.trainUser) + len(self.testUser))/self.n_users/self.m_items}\")\n",
    "        \n",
    "        # (users,users)\n",
    "        self.socialNet    = csr_matrix((np.ones(len(trustNet)), (trustNet[:,0], trustNet[:,1]) ), shape=(self.n_users,self.n_users))\n",
    "        # (users,items), bipartite graph\n",
    "        self.UserItemNet  = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem) ), shape=(self.n_users,self.m_items)) \n",
    "        \n",
    "        # pre-calculate\n",
    "        self._allPos = self.getUserPosItems(list(range(self.n_users)))\n",
    "        self.allNeg = []\n",
    "        allItems    = set(range(self.m_items))\n",
    "        for i in range(self.n_users):\n",
    "            pos = set(self._allPos[i])\n",
    "            neg = allItems - pos\n",
    "            self.allNeg.append(np.array(list(neg)))\n",
    "        self.__testDict = self.__build_test()\n",
    "\n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return 1892\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        return 4489\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        return len(self.trainUser)\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        return self.__testDict\n",
    "\n",
    "    @property\n",
    "    def allPos(self):\n",
    "        return self._allPos\n",
    "\n",
    "    def getSparseGraph(self):\n",
    "        if self.Graph is None:\n",
    "            user_dim = torch.LongTensor(self.trainUser)\n",
    "            item_dim = torch.LongTensor(self.trainItem)\n",
    "            \n",
    "            first_sub = torch.stack([user_dim, item_dim + self.n_users])\n",
    "            second_sub = torch.stack([item_dim+self.n_users, user_dim])\n",
    "            index = torch.cat([first_sub, second_sub], dim=1)\n",
    "            data = torch.ones(index.size(-1)).int()\n",
    "            self.Graph = torch.sparse.IntTensor(index, data, torch.Size([self.n_users+self.m_items, self.n_users+self.m_items]))\n",
    "            dense = self.Graph.to_dense()\n",
    "            D = torch.sum(dense, dim=1).float()\n",
    "            D[D==0.] = 1.\n",
    "            D_sqrt = torch.sqrt(D).unsqueeze(dim=0)\n",
    "            dense = dense/D_sqrt\n",
    "            dense = dense/D_sqrt.t()\n",
    "            index = dense.nonzero()\n",
    "            data  = dense[dense >= 1e-9]\n",
    "            assert len(index) == len(data)\n",
    "            self.Graph = torch.sparse.FloatTensor(index.t(), data, torch.Size([self.n_users+self.m_items, self.n_users+self.m_items]))\n",
    "            self.Graph = self.Graph.coalesce().to(world.device)\n",
    "        return self.Graph\n",
    "\n",
    "    def __build_test(self):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            dict: {user: [items]}\n",
    "        \"\"\"\n",
    "        test_data = {}\n",
    "        for i, item in enumerate(self.testItem):\n",
    "            user = self.testUser[i]\n",
    "            if test_data.get(user):\n",
    "                test_data[user].append(item)\n",
    "            else:\n",
    "                test_data[user] = [item]\n",
    "        return test_data\n",
    "    \n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        \"\"\"\n",
    "        users:\n",
    "            shape [-1]\n",
    "        items:\n",
    "            shape [-1]\n",
    "        return:\n",
    "            feedback [-1]\n",
    "        \"\"\"\n",
    "        # print(self.UserItemNet[users, items])\n",
    "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1, ))\n",
    "    \n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "        return posItems\n",
    "    \n",
    "    def getUserNegItems(self, users):\n",
    "        negItems = []\n",
    "        for user in users:\n",
    "            negItems.append(self.allNeg[user])\n",
    "        return negItems\n",
    "            \n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user = self.trainUniqueUsers[index]\n",
    "        # return user_id and the positive items of the user\n",
    "        return user\n",
    "    \n",
    "    def switch2test(self):\n",
    "        \"\"\"\n",
    "        change dataset mode to offer test data to dataloader\n",
    "        \"\"\"\n",
    "        self.mode = self.mode_dict['test']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trainUniqueUsers)\n",
    "\n",
    "class Loader(BasicDataset):\n",
    "    \"\"\"\n",
    "    Dataset type for pytorch \\n\n",
    "    Incldue graph information\n",
    "    gowalla dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,config = world.config,path=\"../data/gowalla\"):\n",
    "        # train or test\n",
    "        cprint(f'loading [{path}]')\n",
    "        self.split = config['A_split']\n",
    "        self.folds = config['A_n_fold']\n",
    "        self.mode_dict = {'train': 0, \"test\": 1}\n",
    "        self.mode = self.mode_dict['train']\n",
    "        self.n_user = 0\n",
    "        self.m_item = 0\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "        self.path = path\n",
    "        trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "        testUniqueUsers, testItem, testUser = [], [], []\n",
    "        self.traindataSize = 0\n",
    "        self.testDataSize = 0\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.traindataSize += len(items)\n",
    "        self.trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "        self.trainUser = np.array(trainUser)\n",
    "        self.trainItem = np.array(trainItem)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    testUniqueUsers.append(uid)\n",
    "                    testUser.extend([uid] * len(items))\n",
    "                    testItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.testDataSize += len(items)\n",
    "        self.m_item += 1\n",
    "        self.n_user += 1\n",
    "        self.testUniqueUsers = np.array(testUniqueUsers)\n",
    "        self.testUser = np.array(testUser)\n",
    "        self.testItem = np.array(testItem)\n",
    "        \n",
    "        self.Graph = None\n",
    "        print(f\"{self.trainDataSize} interactions for training\")\n",
    "        print(f\"{self.testDataSize} interactions for testing\")\n",
    "        print(f\"{world.dataset} Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n",
    "\n",
    "        # (users,items), bipartite graph\n",
    "        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n",
    "                                      shape=(self.n_user, self.m_item))\n",
    "        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n",
    "        self.users_D[self.users_D == 0.] = 1\n",
    "        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n",
    "        self.items_D[self.items_D == 0.] = 1.\n",
    "        # pre-calculate\n",
    "        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n",
    "        self.__testDict = self.__build_test()\n",
    "        print(f\"{world.dataset} is ready to go\")\n",
    "\n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return self.n_user\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        return self.m_item\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        return self.traindataSize\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        return self.__testDict\n",
    "\n",
    "    @property\n",
    "    def allPos(self):\n",
    "        return self._allPos\n",
    "\n",
    "    def _split_A_hat(self,A):\n",
    "        A_fold = []\n",
    "        fold_len = (self.n_users + self.m_items) // self.folds\n",
    "        for i_fold in range(self.folds):\n",
    "            start = i_fold*fold_len\n",
    "            if i_fold == self.folds - 1:\n",
    "                end = self.n_users + self.m_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(world.device))\n",
    "        return A_fold\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        row = torch.Tensor(coo.row).long()\n",
    "        col = torch.Tensor(coo.col).long()\n",
    "        index = torch.stack([row, col])\n",
    "        data = torch.FloatTensor(coo.data)\n",
    "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "        \n",
    "    def getSparseGraph(self):\n",
    "        print(\"loading adjacency matrix\")\n",
    "        if self.Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n",
    "                print(\"successfully loaded...\")\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                s = time()\n",
    "                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = self.UserItemNet.tolil()\n",
    "                adj_mat[:self.n_users, self.n_users:] = R\n",
    "                adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat)\n",
    "                norm_adj = norm_adj.dot(d_mat)\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                end = time()\n",
    "                print(f\"costing {end-s}s, saved norm_mat...\")\n",
    "                sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = self._split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(world.device)\n",
    "                print(\"don't split the matrix\")\n",
    "        return self.Graph\n",
    "\n",
    "    def __build_test(self):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            dict: {user: [items]}\n",
    "        \"\"\"\n",
    "        test_data = {}\n",
    "        for i, item in enumerate(self.testItem):\n",
    "            user = self.testUser[i]\n",
    "            if test_data.get(user):\n",
    "                test_data[user].append(item)\n",
    "            else:\n",
    "                test_data[user] = [item]\n",
    "        return test_data\n",
    "\n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        \"\"\"\n",
    "        users:\n",
    "            shape [-1]\n",
    "        items:\n",
    "            shape [-1]\n",
    "        return:\n",
    "            feedback [-1]\n",
    "        \"\"\"\n",
    "        # print(self.UserItemNet[users, items])\n",
    "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n",
    "\n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "        return posItems\n",
    "\n",
    "    # def getUserNegItems(self, users):\n",
    "    #     negItems = []\n",
    "    #     for user in users:\n",
    "    #         negItems.append(self.allNeg[user])\n",
    "    #     return negItems\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
