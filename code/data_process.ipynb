{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"init dataset\")\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def allPos(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserPosItems(self, users):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserNegItems(self, users):\n",
    "        \"\"\"\n",
    "        not necessary for large dataset\n",
    "        it's stupid to return all neg items in super large dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getSparseGraph(self):\n",
    "        \"\"\"\n",
    "        build a graph in torch.sparse.IntTensor.\n",
    "        Details in NGCF's matrix form\n",
    "        A = \n",
    "            |I,   R|\n",
    "            |R^T, I|\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = input('ml100k/movies.csv')\n",
    "txt_file = input('ml100k/movies_list.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-31-ea513b44d9cb>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-ea513b44d9cb>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    [ my_output_file.write(\" \".join(row)+'\\n') for row+100 in csv.reader(my_input_file)]\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv_file = input('ml100k/ratings.csv')\n",
    "txt_file = input('ml100k/train.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = input('ml100k/user_list.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    for i in range(609):\n",
    "        \n",
    "        [ my_output_file.write(\"user_\"+ str(i+1) +\" \"+ str(i+1) +'\\n')]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/Users/sayamsingla/Desktop/Warwick/LightGCN-PyTorch-master/data/gowalla/train.txt'\n",
    "train_file2 = '/Users/sayamsingla/Desktop/Warwick/LightGCN-PyTorch-master/data/ml100k/train.txt'\n",
    "path = '/Users/sayamsingla/Desktop/Warwick/LightGCN-PyTorch-master/code/ml100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainUniqueUsers, trainItem, trainUser , trainRating= [], [], [], []\n",
    "testUniqueUsers, testItem, testUser = [], [], []\n",
    "trainDataSize = 0\n",
    "testDataSize = 0\n",
    "n_user = 0\n",
    "m_item = 0\n",
    "ref = 0\n",
    "c = []\n",
    "\n",
    "with open(train_file2) as f:\n",
    "            for l in f.readlines():\n",
    "                \n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    #items = [int(i) for i in l[1]]\n",
    "                    items = int(l[1])\n",
    "                    rating = float(str(l[2])) \n",
    "                    \n",
    "                    \n",
    "                    uid = int(l[0])\n",
    "                    \n",
    "                    if uid > ref:\n",
    "                        trainUniqueUsers.append(uid)\n",
    "                        ref = uid\n",
    "                    \n",
    "                    trainRating.append(rating)\n",
    "                    #trainUser.extend([uid] * len(items))\n",
    "                    trainUser.append(uid)\n",
    "                    #trainItem.extend(items)\n",
    "                    trainItem.append(items)\n",
    "                    m_item = max(m_item, items)\n",
    "                    n_user = max(n_user, uid)\n",
    "                    trainDataSize += 1\n",
    "c = trainItem\n",
    "m_item = len(np.array(list(set(c))))\n",
    "\n",
    "trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "trainUser = np.array(trainUser)\n",
    "trainItem = np.array(trainItem)\n",
    "trainRating = np.array(trainRating)\n",
    "m_item = trainItem.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,   1,   1, ..., 610, 610, 610]),\n",
       " array([     1,      3,      6, ..., 168250, 168252, 170875]),\n",
       " 193609)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = trainItem.max()\n",
    "trainUser, trainItem, m_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100836,), (100836,), (100836,), 193609, 610, 100836)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainItem.shape , trainUser.shape , trainRating.shape , m_item, n_user , trainDataSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29858"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "testUniqueUsers, testItem, testUser = [], [], []\n",
    "trainDataSize = 0\n",
    "testDataSize = 0\n",
    "n_user = 0\n",
    "m_item = 0\n",
    "c =   0\n",
    "with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                c +=1\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    m_item = max(m_item, max(items))\n",
    "                    n_user = max(n_user, uid)\n",
    "                    trainDataSize += len(items)\n",
    "                    \n",
    "trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "trainUser = np.array(trainUser)\n",
    "trainItem = np.array(trainItem)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     0,     0, ..., 29857, 29857, 29857]),\n",
       " array([   0,    1,    2, ..., 1853,  691,  674]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUser, trainItem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810128,), 810128)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainItem.shape, len(trainUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserPosItems( users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(UserItemNet[user].nonzero()[1])\n",
    "        return posItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100836 interactions for training\n",
      "0 interactions for testing\n",
      "Sparsity : 0.0008524062284253587\n",
      "  (1, 1)\t4.0\n",
      "  (1, 3)\t4.0\n",
      "  (1, 6)\t4.0\n",
      "  (1, 47)\t5.0\n",
      "  (1, 50)\t5.0\n",
      "  (1, 70)\t3.0\n",
      "  (1, 101)\t5.0\n",
      "  (1, 110)\t4.0\n",
      "  (1, 151)\t5.0\n",
      "  (1, 157)\t5.0\n",
      "  (1, 163)\t5.0\n",
      "  (1, 216)\t5.0\n",
      "  (1, 223)\t3.0\n",
      "  (1, 231)\t5.0\n",
      "  (1, 235)\t4.0\n",
      "  (1, 260)\t5.0\n",
      "  (1, 296)\t3.0\n",
      "  (1, 316)\t3.0\n",
      "  (1, 333)\t5.0\n",
      "  (1, 349)\t4.0\n",
      "  (1, 356)\t4.0\n",
      "  (1, 362)\t5.0\n",
      "  (1, 367)\t4.0\n",
      "  (1, 423)\t3.0\n",
      "  (1, 441)\t4.0\n",
      "  :\t:\n",
      "  (610, 156371)\t5.0\n",
      "  (610, 156726)\t4.5\n",
      "  (610, 157296)\t4.0\n",
      "  (610, 158238)\t5.0\n",
      "  (610, 158721)\t3.5\n",
      "  (610, 158872)\t3.5\n",
      "  (610, 158956)\t3.0\n",
      "  (610, 159093)\t3.0\n",
      "  (610, 160080)\t3.0\n",
      "  (610, 160341)\t2.5\n",
      "  (610, 160527)\t4.5\n",
      "  (610, 160571)\t3.0\n",
      "  (610, 160836)\t3.0\n",
      "  (610, 161582)\t4.0\n",
      "  (610, 161634)\t4.0\n",
      "  (610, 162350)\t3.5\n",
      "  (610, 163937)\t3.5\n",
      "  (610, 163981)\t3.5\n",
      "  (610, 164179)\t5.0\n",
      "  (610, 166528)\t4.0\n",
      "  (610, 166534)\t4.0\n",
      "  (610, 168248)\t5.0\n",
      "  (610, 168250)\t5.0\n",
      "  (610, 168252)\t5.0\n",
      "  (610, 170875)\t3.0\n"
     ]
    }
   ],
   "source": [
    "        Graph = None\n",
    "        print(f\"{trainDataSize} interactions for training\")\n",
    "        print(f\"{testDataSize} interactions for testing\")\n",
    "        print(f\"Sparsity : {(trainDataSize + testDataSize) / n_user / m_item}\")\n",
    "        m_item += 1\n",
    "        n_user += 1\n",
    "        #print(n_user, size)\n",
    "        # (users,items), bipartite graph\n",
    "       # UserItemNet = csr_matrix((np.ones(len(trainUser)), (trainUser, trainItem)),shape=(n_user, m_item))\n",
    "        UserItemNet = csr_matrix((trainRating, (trainUser, trainItem)),shape=(n_user, m_item))\n",
    "        print(UserItemNet)\n",
    "        \n",
    "        users_D = np.array(UserItemNet.sum(axis=1)).squeeze()\n",
    "        users_D[users_D == 0.] = 1\n",
    "        items_D = np.array(UserItemNet.sum(axis=0)).squeeze()\n",
    "        items_D[items_D == 0.] = 1.\n",
    "        # pre-calculate\n",
    "        _allPos = getUserPosItems(list(range(n_user)))\n",
    "        #__testDict = __build_test()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = n_user\n",
    "m_items = m_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating adjacency matrix\n",
      "costing s, saved norm_mat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayamsingla/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c3f9c7f10c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/s_pre_adj_mat.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_A_hat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done split matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        if Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(path + '/s_pre_adj_mat.npz')\n",
    "                print(\"successfully loaded...\")\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                #s = time()\n",
    "                adj_mat = sp.dok_matrix((n_users + m_items, n_users + m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = UserItemNet.tolil()\n",
    "                adj_mat[:n_users, n_users:] = R\n",
    "                adj_mat[n_users:, :n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat)\n",
    "                norm_adj = norm_adj.dot(d_mat)\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                #end = time()\n",
    "                print(f\"costing s, saved norm_mat...\")\n",
    "                sp.save_npz(path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = _split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(world.device)\n",
    "                print(\"don't split the matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(BasicDataset):\n",
    "    \"\"\"\n",
    "    Dataset type for pytorch \\n\n",
    "    Incldue graph information\n",
    "    gowalla dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path=\"../data/gowalla\"):\n",
    "        # train or test\n",
    "       \n",
    "        self.mode_dict = {'train': 0, \"test\": 1}\n",
    "        self.mode = self.mode_dict['train']\n",
    "        self.n_user = 0\n",
    "        self.m_item = 0\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "        self.path = path\n",
    "        trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "        testUniqueUsers, testItem, testUser = [], [], []\n",
    "        self.traindataSize = 0\n",
    "        self.testDataSize = 0\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.traindataSize += len(items)\n",
    "        self.trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "        self.trainUser = np.array(trainUser)\n",
    "        self.trainItem = np.array(trainItem)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    testUniqueUsers.append(uid)\n",
    "                    testUser.extend([uid] * len(items))\n",
    "                    testItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.testDataSize += len(items)\n",
    "        self.m_item += 1\n",
    "        self.n_user += 1\n",
    "        self.testUniqueUsers = np.array(testUniqueUsers)\n",
    "        self.testUser = np.array(testUser)\n",
    "        self.testItem = np.array(testItem)\n",
    "        \n",
    "        self.Graph = None\n",
    "        print(f\"{self.trainDataSize} interactions for training\")\n",
    "        print(f\"{self.testDataSize} interactions for testing\")\n",
    "        print(f\"Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n",
    "\n",
    "        # (users,items), bipartite graph\n",
    "        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n",
    "                                      shape=(self.n_user, self.m_item))\n",
    "        print(self.n_user, self.m_item)\n",
    "        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n",
    "        self.users_D[self.users_D == 0.] = 1\n",
    "        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n",
    "        self.items_D[self.items_D == 0.] = 1.\n",
    "        # pre-calculate\n",
    "        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n",
    "        self.__testDict = self.__build_test()\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return self.n_user\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        return self.m_item\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        return self.traindataSize\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        return self.__testDict\n",
    "\n",
    "    @property\n",
    "    def allPos(self):\n",
    "        return self._allPos\n",
    "\n",
    "    def _split_A_hat(self,A):\n",
    "        A_fold = []\n",
    "        fold_len = (self.n_users + self.m_items) // self.folds\n",
    "        for i_fold in range(self.folds):\n",
    "            start = i_fold*fold_len\n",
    "            if i_fold == self.folds - 1:\n",
    "                end = self.n_users + self.m_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(world.device))\n",
    "        return A_fold\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        row = torch.Tensor(coo.row).long()\n",
    "        col = torch.Tensor(coo.col).long()\n",
    "        index = torch.stack([row, col])\n",
    "        data = torch.FloatTensor(coo.data)\n",
    "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "        \n",
    "    def getSparseGraph(self):\n",
    "        print(\"loading adjacency matrix\")\n",
    "        if self.Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n",
    "                print(\"successfully loaded...\")\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                s = time()\n",
    "                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = self.UserItemNet.tolil()\n",
    "                adj_mat[:self.n_users, self.n_users:] = R\n",
    "                adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat)\n",
    "                norm_adj = norm_adj.dot(d_mat)\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                end = time()\n",
    "                print(f\"costing {end-s}s, saved norm_mat...\")\n",
    "                sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = self._split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(world.device)\n",
    "                print(\"don't split the matrix\")\n",
    "        return self.Graph\n",
    "\n",
    "    def __build_test(self):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            dict: {user: [items]}\n",
    "        \"\"\"\n",
    "        test_data = {}\n",
    "        for i, item in enumerate(self.testItem):\n",
    "            user = self.testUser[i]\n",
    "            if test_data.get(user):\n",
    "                test_data[user].append(item)\n",
    "            else:\n",
    "                test_data[user] = [item]\n",
    "        return test_data\n",
    "\n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        \"\"\"\n",
    "        users:\n",
    "            shape [-1]\n",
    "        items:\n",
    "            shape [-1]\n",
    "        return:\n",
    "            feedback [-1]\n",
    "        \"\"\"\n",
    "        # print(self.UserItemNet[users, items])\n",
    "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n",
    "\n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "        return posItems\n",
    "\n",
    "    # def getUserNegItems(self, users):\n",
    "    #     negItems = []\n",
    "    #     for user in users:\n",
    "    #         negItems.append(self.allNeg[user])\n",
    "    #     return negItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " l = Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
