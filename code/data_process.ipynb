{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        print(\"init dataset\")\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def allPos(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserPosItems(self, users):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getUserNegItems(self, users):\n",
    "        \"\"\"\n",
    "        not necessary for large dataset\n",
    "        it's stupid to return all neg items in super large dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def getSparseGraph(self):\n",
    "        \"\"\"\n",
    "        build a graph in torch.sparse.IntTensor.\n",
    "        Details in NGCF's matrix form\n",
    "        A = \n",
    "            |I,   R|\n",
    "            |R^T, I|\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = input('ml100k/movies.csv')\n",
    "txt_file = input('ml100k/movies_list.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = input('ml100k/ratings.csv')\n",
    "txt_file = input('ml100k/train.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = input('ml100k/user_list.txt')\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    for i in range(609):\n",
    "        \n",
    "        [ my_output_file.write(\"user_\"+ str(i+1) +\" \"+ str(i+1) +'\\n')]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/Users/sayamsingla/Desktop/Warwick/LightGCN-Movie/LightGCN_MovieLens/data/gowalla/train.txt'\n",
    "train_file2 = '/Users/sayamsingla/Desktop/Warwick/LightGCN-Movie/LightGCN_MovieLens/data/ml100k/train.txt'\n",
    "train_file3 = '/Users/sayamsingla/Desktop/Warwick/LightGCN-Movie/LightGCN_MovieLens/data/ml100k/map.txt'\n",
    "path = '/Users/sayamsingla/Desktop/Warwick/LightGCN-Movie/LightGCN_MovieLens/code/ml100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainUniqueUsers, trainItem, trainUser , trainRating= [], [], [], []\n",
    "testUniqueUsers, testItem, testUser = [], [], []\n",
    "trainDataSize = 0\n",
    "testDataSize = 0\n",
    "n_user = 0\n",
    "m_item = 0\n",
    "ref = 0\n",
    "c = []\n",
    "map = [0]*193610\n",
    "a = 1\n",
    "with open(train_file3) as f:\n",
    "    \n",
    "            for l in f.readlines():\n",
    "                \n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    #items = [int(i) for i in l[1]]\n",
    "                    map_items = int(l[1])\n",
    "                    \n",
    "                    if map[map_items] == 0:\n",
    "                        map[map_items] = a\n",
    "                        a+=1\n",
    "                    \n",
    "\n",
    "with open(train_file2) as f:\n",
    "            for l in f.readlines():\n",
    "                \n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    #items = [int(i) for i in l[1]]\n",
    "                    items = int(l[1])\n",
    "                    rating = float(str(l[2])) \n",
    "                    \n",
    "                    \n",
    "                    uid = int(l[0])\n",
    "                    \n",
    "                    if uid > ref:\n",
    "                        trainUniqueUsers.append(uid)\n",
    "                        ref = uid\n",
    "                    \n",
    "                    trainItem.append(map[items])\n",
    "                    trainRating.append(rating)\n",
    "                    trainUser.append(uid)\n",
    "                    #trainItem.extend(items)\n",
    "                    #trainItem.append(items)\n",
    "                    m_item = max(m_item, items)\n",
    "                    n_user = max(n_user, uid)\n",
    "                    trainDataSize += 1\n",
    "c = trainItem\n",
    "m_item = len(np.array(list(set(c))))\n",
    "\n",
    "trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "trainUser = np.array(trainUser)\n",
    "trainItem = np.array(trainItem)\n",
    "trainRating = np.array(trainRating)\n",
    "m_item = max(trainItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,   1,   1, ..., 610, 610, 610]),\n",
       " array([   1,    3,    6, ..., 9445, 9446, 9486]),\n",
       " 9724)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = trainItem.max()\n",
    "trainUser, trainItem, m_item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100836,), (100836,), (100836,), 9724, 610, 100836)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainItem.shape , trainUser.shape , trainRating.shape , m_item, n_user , trainDataSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29858"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "testUniqueUsers, testItem, testUser = [], [], []\n",
    "trainDataSize = 0\n",
    "testDataSize = 0\n",
    "n_user = 0\n",
    "m_item = 0\n",
    "c =   0\n",
    "with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                c +=1\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    m_item = max(m_item, max(items))\n",
    "                    n_user = max(n_user, uid)\n",
    "                    trainDataSize += len(items)\n",
    "                    \n",
    "trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "trainUser = np.array(trainUser)\n",
    "trainItem = np.array(trainItem)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    trainUser, trainItem, m_item\n",
    "    arr = trainItem\n",
    "    res = 1\n",
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((810128,), 810128, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainItem.shape, len(trainUser), res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserPosItems( users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(UserItemNet[user].nonzero()[1])\n",
    "        return posItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100836 interactions for training\n",
      "0 interactions for testing\n",
      "Sparsity : 0.016999683055613623\n",
      "  (1, 1)\t1.0\n",
      "  (1, 3)\t1.0\n",
      "  (1, 6)\t1.0\n",
      "  (1, 44)\t1.0\n",
      "  (1, 47)\t1.0\n",
      "  (1, 63)\t1.0\n",
      "  (1, 90)\t1.0\n",
      "  (1, 98)\t1.0\n",
      "  (1, 125)\t1.0\n",
      "  (1, 131)\t1.0\n",
      "  (1, 137)\t1.0\n",
      "  (1, 185)\t1.0\n",
      "  (1, 191)\t1.0\n",
      "  (1, 198)\t1.0\n",
      "  (1, 202)\t1.0\n",
      "  (1, 225)\t1.0\n",
      "  (1, 258)\t1.0\n",
      "  (1, 276)\t1.0\n",
      "  (1, 292)\t1.0\n",
      "  (1, 308)\t1.0\n",
      "  (1, 315)\t1.0\n",
      "  (1, 321)\t1.0\n",
      "  (1, 326)\t1.0\n",
      "  (1, 368)\t1.0\n",
      "  (1, 385)\t1.0\n",
      "  :\t:\n",
      "  (610, 9239)\t1.0\n",
      "  (610, 9247)\t1.0\n",
      "  (610, 9257)\t1.0\n",
      "  (610, 9269)\t1.0\n",
      "  (610, 9275)\t1.0\n",
      "  (610, 9280)\t1.0\n",
      "  (610, 9283)\t1.0\n",
      "  (610, 9289)\t1.0\n",
      "  (610, 9305)\t1.0\n",
      "  (610, 9308)\t1.0\n",
      "  (610, 9313)\t1.0\n",
      "  (610, 9318)\t1.0\n",
      "  (610, 9325)\t1.0\n",
      "  (610, 9340)\t1.0\n",
      "  (610, 9342)\t1.0\n",
      "  (610, 9349)\t1.0\n",
      "  (610, 9372)\t1.0\n",
      "  (610, 9373)\t1.0\n",
      "  (610, 9375)\t1.0\n",
      "  (610, 9416)\t1.0\n",
      "  (610, 9417)\t1.0\n",
      "  (610, 9444)\t1.0\n",
      "  (610, 9445)\t1.0\n",
      "  (610, 9446)\t1.0\n",
      "  (610, 9486)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        Graph = None\n",
    "        print(f\"{trainDataSize} interactions for training\")\n",
    "        print(f\"{testDataSize} interactions for testing\")\n",
    "        print(f\"Sparsity : {(trainDataSize + testDataSize) / n_user / m_item}\")\n",
    "        m_item += 1\n",
    "        n_user += 1\n",
    "        #print(n_user, size)\n",
    "        # (users,items), bipartite graph\n",
    "        UserItemNet = csr_matrix((np.ones(len(trainUser)), (trainUser, trainItem)),shape=(n_user, m_item))\n",
    "#         UserItemNet = csr_matrix((trainRating, (trainUser, trainItem)),shape=(n_user, m_item))\n",
    "        print(UserItemNet)\n",
    "        \n",
    "        users_D = np.array(UserItemNet.sum(axis=1)).squeeze()\n",
    "        users_D[users_D == 0.] = 1\n",
    "        items_D = np.array(UserItemNet.sum(axis=0)).squeeze()\n",
    "        items_D[items_D == 0.] = 1.\n",
    "        # pre-calculate\n",
    "        _allPos = getUserPosItems(list(range(n_user)))\n",
    "        #__testDict = __build_test()\n",
    "        len(_allPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = n_user\n",
    "m_items = m_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(path + '/s_pre_adj_mat.npz')\n",
    "                print(\"successfully loaded...\")\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                #s = time()\n",
    "                adj_mat = sp.dok_matrix((n_users + m_items, n_users + m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = UserItemNet.tolil()\n",
    "                adj_mat[:n_users, n_users:] = R\n",
    "                adj_mat[n_users:, :n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat)\n",
    "                norm_adj = norm_adj.dot(d_mat)\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                #end = time()\n",
    "                print(f\"costing s, saved norm_mat...\")\n",
    "                sp.save_npz(path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = _split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(world.device)\n",
    "                print(\"don't split the matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(BasicDataset):\n",
    "    \"\"\"\n",
    "    Dataset type for pytorch \\n\n",
    "    Incldue graph information\n",
    "    gowalla dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path=\"../data/gowalla\"):\n",
    "        # train or test\n",
    "       \n",
    "        self.mode_dict = {'train': 0, \"test\": 1}\n",
    "        self.mode = self.mode_dict['train']\n",
    "        self.n_user = 0\n",
    "        self.m_item = 0\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "        self.path = path\n",
    "        trainUniqueUsers, trainItem, trainUser = [], [], []\n",
    "        testUniqueUsers, testItem, testUser = [], [], []\n",
    "        self.traindataSize = 0\n",
    "        self.testDataSize = 0\n",
    "\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    trainUniqueUsers.append(uid)\n",
    "                    trainUser.extend([uid] * len(items))\n",
    "                    trainItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.traindataSize += len(items)\n",
    "        self.trainUniqueUsers = np.array(trainUniqueUsers)\n",
    "        self.trainUser = np.array(trainUser)\n",
    "        self.trainItem = np.array(trainItem)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    l = l.strip('\\n').split(' ')\n",
    "                    items = [int(i) for i in l[1:]]\n",
    "                    uid = int(l[0])\n",
    "                    testUniqueUsers.append(uid)\n",
    "                    testUser.extend([uid] * len(items))\n",
    "                    testItem.extend(items)\n",
    "                    self.m_item = max(self.m_item, max(items))\n",
    "                    self.n_user = max(self.n_user, uid)\n",
    "                    self.testDataSize += len(items)\n",
    "        self.m_item += 1\n",
    "        self.n_user += 1\n",
    "        self.testUniqueUsers = np.array(testUniqueUsers)\n",
    "        self.testUser = np.array(testUser)\n",
    "        self.testItem = np.array(testItem)\n",
    "        \n",
    "        self.Graph = None\n",
    "        print(f\"{self.trainDataSize} interactions for training\")\n",
    "        print(f\"{self.testDataSize} interactions for testing\")\n",
    "        print(f\"Sparsity : {(self.trainDataSize + self.testDataSize) / self.n_users / self.m_items}\")\n",
    "\n",
    "        # (users,items), bipartite graph\n",
    "        self.UserItemNet = csr_matrix((np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),\n",
    "                                      shape=(self.n_user, self.m_item))\n",
    "        print(self.n_user, self.m_item)\n",
    "        self.users_D = np.array(self.UserItemNet.sum(axis=1)).squeeze()\n",
    "        self.users_D[self.users_D == 0.] = 1\n",
    "        self.items_D = np.array(self.UserItemNet.sum(axis=0)).squeeze()\n",
    "        self.items_D[self.items_D == 0.] = 1.\n",
    "        # pre-calculate\n",
    "        self._allPos = self.getUserPosItems(list(range(self.n_user)))\n",
    "        self.__testDict = self.__build_test()\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return self.n_user\n",
    "    \n",
    "    @property\n",
    "    def m_items(self):\n",
    "        return self.m_item\n",
    "    \n",
    "    @property\n",
    "    def trainDataSize(self):\n",
    "        return self.traindataSize\n",
    "    \n",
    "    @property\n",
    "    def testDict(self):\n",
    "        return self.__testDict\n",
    "\n",
    "    @property\n",
    "    def allPos(self):\n",
    "        return self._allPos\n",
    "\n",
    "    def _split_A_hat(self,A):\n",
    "        A_fold = []\n",
    "        fold_len = (self.n_users + self.m_items) // self.folds\n",
    "        for i_fold in range(self.folds):\n",
    "            start = i_fold*fold_len\n",
    "            if i_fold == self.folds - 1:\n",
    "                end = self.n_users + self.m_items\n",
    "            else:\n",
    "                end = (i_fold + 1) * fold_len\n",
    "            A_fold.append(self._convert_sp_mat_to_sp_tensor(A[start:end]).coalesce().to(world.device))\n",
    "        return A_fold\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        row = torch.Tensor(coo.row).long()\n",
    "        col = torch.Tensor(coo.col).long()\n",
    "        index = torch.stack([row, col])\n",
    "        data = torch.FloatTensor(coo.data)\n",
    "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "        \n",
    "    def getSparseGraph(self):\n",
    "        print(\"loading adjacency matrix\")\n",
    "        if self.Graph is None:\n",
    "            try:\n",
    "                pre_adj_mat = sp.load_npz(self.path + '/s_pre_adj_mat.npz')\n",
    "                print(\"successfully loaded...\")\n",
    "                norm_adj = pre_adj_mat\n",
    "            except :\n",
    "                print(\"generating adjacency matrix\")\n",
    "                s = time()\n",
    "                adj_mat = sp.dok_matrix((self.n_users + self.m_items, self.n_users + self.m_items), dtype=np.float32)\n",
    "                adj_mat = adj_mat.tolil()\n",
    "                R = self.UserItemNet.tolil()\n",
    "                adj_mat[:self.n_users, self.n_users:] = R\n",
    "                adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "                adj_mat = adj_mat.todok()\n",
    "                # adj_mat = adj_mat + sp.eye(adj_mat.shape[0])\n",
    "                \n",
    "                rowsum = np.array(adj_mat.sum(axis=1))\n",
    "                d_inv = np.power(rowsum, -0.5).flatten()\n",
    "                d_inv[np.isinf(d_inv)] = 0.\n",
    "                d_mat = sp.diags(d_inv)\n",
    "                \n",
    "                norm_adj = d_mat.dot(adj_mat)\n",
    "                norm_adj = norm_adj.dot(d_mat)\n",
    "                norm_adj = norm_adj.tocsr()\n",
    "                end = time()\n",
    "                print(f\"costing {end-s}s, saved norm_mat...\")\n",
    "                sp.save_npz(self.path + '/s_pre_adj_mat.npz', norm_adj)\n",
    "\n",
    "            if self.split == True:\n",
    "                self.Graph = self._split_A_hat(norm_adj)\n",
    "                print(\"done split matrix\")\n",
    "            else:\n",
    "                self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "                self.Graph = self.Graph.coalesce().to(world.device)\n",
    "                print(\"don't split the matrix\")\n",
    "        return self.Graph\n",
    "\n",
    "    def __build_test(self):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            dict: {user: [items]}\n",
    "        \"\"\"\n",
    "        test_data = {}\n",
    "        for i, item in enumerate(self.testItem):\n",
    "            user = self.testUser[i]\n",
    "            if test_data.get(user):\n",
    "                test_data[user].append(item)\n",
    "            else:\n",
    "                test_data[user] = [item]\n",
    "        return test_data\n",
    "\n",
    "    def getUserItemFeedback(self, users, items):\n",
    "        \"\"\"\n",
    "        users:\n",
    "            shape [-1]\n",
    "        items:\n",
    "            shape [-1]\n",
    "        return:\n",
    "            feedback [-1]\n",
    "        \"\"\"\n",
    "        # print(self.UserItemNet[users, items])\n",
    "        return np.array(self.UserItemNet[users, items]).astype('uint8').reshape((-1,))\n",
    "\n",
    "    def getUserPosItems(self, users):\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "        return posItems\n",
    "\n",
    "    # def getUserNegItems(self, users):\n",
    "    #     negItems = []\n",
    "    #     for user in users:\n",
    "    #         negItems.append(self.allNeg[user])\n",
    "    #     return negItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " l = Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
